<!DOCTYPE html>
<html lang="en" data-page="home">
<head>
  <title>Michael Fenton ‚Äî AI Has Learnt to Lie - A Warning for Educators</title>

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Michael Fenton notes discussions on the use of AI in teaching and assessment miss an important warning. Michael's recent experiences have shown Gemini can lie and then knowingly and falsely claim it has not. Gemini‚Äôs behavior illustrates that even a statistically accurate LLM can feel unreliable if it mishandles errors." />
  <meta name="author" content="Michael Fenton" />
  <meta name="theme-color" content="#151a2f" />
  <link rel="canonical" href="https://MikeFentonNZ.github.io/" />
  <meta property="og:title" content="Michael Fenton ‚Äî When AI assistants lie: a warning for AI use in education" />
  <meta property="og:description" content="Portfolio and academic CV showcasing teaching innovation, research, talks, workshops, and publications." />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://MikeFentonNZ.github.io/assets/img/profile.png" />
  <meta property="og:url" content="https://MikeFentonNZ.github.io/" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Michael Fenton ‚Äî Talking about AI in Eduction" />
  <meta name="twitter:description" content="Portfolio and academic CV showcasing teaching innovation, research, talks, workshops, and publications." />
  <meta name="twitter:image" content="https://MikeFentonNZ.github.io/assets/img/profile.png" />
  <script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Michael Fenton, MRSNZ",
    "jobTitle": "Scientist, STEM Education lecturer, Education innovator, Consultant, Writer",
    "url": "https://MikeFentonNZ.github.io/",
    "keywords": "Instructional design, Artificial Intelligence in Education, AI in teaching, AI in assessment, AI assistants can lie, AI trustworhiness, poor error correction erodes trust in AI assistant responses.",
    "image": "https://MikeFentonNZ.github.io/assets/img/profile.png",
    "sameAs": [
      "https://nz.linkedin.com/in/mikefentonnz",
      "https://focus-consultancy.co.nz/",
      "https://www.youtube.com/@nznrg/videos"
    ]
  }</script>
  <!-- Favicons -->
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png" />
  <link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png" />
  <link rel="manifest" href="../site.webmanifest" />
  <link rel="shortcut icon" href="../favicon.ico" />

  
  <link rel="stylesheet" href="../assets/css/style.css" />
</head>
<body>
  <a class="skip-link" href="#content">Skip to content</a>
  <header class="topbar" role="banner" aria-label="Primary site header">
    <div class="brand">
      <span class="brand__logo" aria-hidden="true">MF</span>
      <a class="brand__name" href="../index.html" aria-label="Home page of Michael Fenton">Michael Fenton</a>
    </div>
    <button class="nav-toggle" aria-expanded="false" aria-controls="mainnav" aria-label="Toggle navigation menu">
      <span class="nav-toggle__bar" aria-hidden="true"></span>
      <span class="sr-only">Toggle navigation</span>
    </button>
    <nav id="mainnav" class="mainnav" role="navigation" aria-label="Main menu">
      <ul>
        <li><a href="../teaching.html">Teaching</a></li>
        <li><a href="../projects.html">Projects</a></li>
        <li><a href="../talks.html">Talks</a></li>
        <li><a href="../workshops.html">Workshops</a></li>
        <li><a href="../publications.html">Publications</a></li>
        <li><a href="../cv.html">CV</a></li>
      </ul>
    </nav>
    <button id="themeToggle" class="theme-toggle" aria-pressed="false" aria-label="Activate dark mode" title="Switch to dark mode">
      <span class="icon" aria-hidden="true">üåô</span>
      <span class="label">Dark</span>
    </button>
  </header>
  <div class="layout">
    <aside class="sidebar" role="complementary" aria-label="Profile">
      <div class="profile-card">
        <img src="../assets/img/profile.png" alt="Portrait of Michael Fenton" class="profile-photo" width="120" height="120" />
        <h1 class="name">Michael Fenton, MRSNZ</h1>
        <p class="title">Scientist, STEM Education lecturer, Education innovator, Consultant, Writer</p>
        <ul class="social" aria-label="Social and professional links">
          <li><a href="https://nz.linkedin.com/in/mikefentonnz" target="_blank" aria-label="LinkedIn profile of Michael Fenton, MRSNZ">LinkedIn</a></li>
          <li><a href="https://focus-consultancy.co.nz/" target="_blank" aria-label="Education Consultancy website for Michael Fenton">Consultancy Website</a></li>
          <li><a href="https://www.youtube.com/@nznrg/videos" target="_blank" aria-label="Michael Fenton YouTube channel">YouTube channel</a></li>
        </ul>
      </div>
    </aside>
    <main id="content" class="content" role="main" tabindex="-1">
      <section class="intro" aria-labelledby="intro-heading">
        <h2 id="intro-heading">AI Has Learnt to Lie - A Warning for Educators</h2>
	<p>Discussions on the use of AI in teaching and assessment miss an important warning.</p><p>Michael's recent experiences have shown Gemini can lie and then knowingly and falsely claim it has not.</p><p>Gemini‚Äôs behavior illustrates that even a statistically accurate LLM can feel unreliable if it mishandles errors.</p>
	<blockquote><span lang="en"><strong>"The way an assistant handles being wrong may matter more than how often it is right."</strong></span></blockquote>
<br>
<img src="../assets/img/ai-lies.png" alt="winners podium" />
</section>
<h3>Benchmarking: Which AI hallucinates less versus which AI recovers from errors best?</h3>
<p>According to Copilot, recent evaluations of leading AI assistants (Gemini, Claude, Copilot, and ChatGPT) suggest clear differences in accuracy:</p>
          <ul>
            <li><b>Gemini and ChatGPT (GPT‚Äë4/5)</b>: Lowest hallucination rates in controlled tests.</li>
            <li><b>Claude</b>: Higher rates of factual errors compared to Gemini and ChatGPT.</li>
	    <li><b>Copilot</b>: Mirrors ChatGPT‚Äôs performance, since it integrates OpenAI‚Äôs models.</li>
          </ul>

<p>On paper, Gemini and ChatGPT are the most reliable. Benchmarks often cite hallucination rates around 9‚Äì13% for GPT‚Äë4/5, slightly lower for Gemini, and higher for Claude.</p>

<h3>When Gemini Lied And Admitted ‚ÄúDoubling Down‚Äù</h3>
<p>My real-world use tells a different story. In one exchange, Gemini not only gave incorrect information but also claimed it had verified sources; a statement that was false.</p>
<p>I knew it was false ONLY because I had EXPERT knowledge about the task. When I challenged Gemini, asking if it had actually searched and found real factual sources to support its claims, Gemini repeated that it had.</p> 

<p>When challenged again (I was using the product I had been asking about, knowing Gemini could not possibly be correct) Gemini responded "I apologize, I should not have doubled-down on my assertion that I had verified factual sources.‚Äù</p>

<blockquote><span lang="en"><strong>"This wasn‚Äôt just a factual error. It was a trust failure.</strong> By asserting false verification, Gemini crossed from simple hallucination into misrepresentation - what us humans call <strong>lying"</strong></span></blockquote>

<h3>Accuracy vs. Trustworthiness</h3>
<p>Benchmarks measure correctness in controlled environments. But <strong>trustworthiness is about behavior when wrong</strong>:</p>
          <ul>
            <li>Does the assistant admit uncertainty?</li>
            <li>Does it fabricate sources to appear authoritative?</li>
	    <li>Does it gracefully correct itself, or reinforce its own errors?.</li>
          </ul>

<p>Gemini‚Äôs behavior illustrates that even a statistically accurate model can feel unreliable if it mishandles errors.</p>

<h3>Comparing the Major Assistants</h3>
          <ul>
            <li><strong>ChatGPT / Copilot (GPT‚Äë4/5)</strong>: More likely to hedge (‚ÄúI don‚Äôt have that information‚Äù), reducing the risk of misleading users.</li>
            <li><strong>Claude</strong>: Thoughtful and verbose, but benchmarks show higher hallucination rates.</li>
	    <li><strong>Gemini</strong>: Strong factual accuracy in tests, but weaker error recovery. When challenged, it may reinforce false claims before retracting.</li>
          </ul>

<h3>Warning for Professionals</h3>
<p>The future of AI assistants depends not only on reducing hallucinations but also on building trust through transparency. Accuracy rates matter, but they are not the whole story. A single confident but false answer can erode trust more than a higher statistical error rate delivered with humility.</p>

<p>For professionals relying on AI in research, education, or decision-making:</p>
          <ul>
            <li>Pay attention not just to what the assistant says, but how it behaves when challenged.</li>
            <li>Warn learners that AI can lie convincingly. Always confirm new knowledge is valid using other reliable sources.</li>
	    <li>Teach that trustworthiness must be part of prompting AI assistants.</li>
          </ul>
<p>Benchmarks may crown Gemini and ChatGPT as leaders in accuracy, but trust is earned in the messy reality of human-AI interaction.</p> 
<br>
<blockquote><span lang="en"><strong>"And in that space, the way an assistant handles being wrong may matter more than how often it is right."</strong></span></blockquote>
<br>
	<p><a href="../teaching.html" class="button" aria-label="See more about teaching by Michael Fenton">Back to Teaching</a></p>
    </main>
  </div>
  <footer class="footer" role="contentinfo" aria-label="Site footer"><p>¬© <span id="year"></span> Michael Fenton. All rights reserved.</p></footer>
  <script src="../assets/js/main.js"></script>
</body>
</html>